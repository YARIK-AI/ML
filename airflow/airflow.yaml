apiVersion : v1
kind : Service
metadata:
  name: airflow-web
  labels:
    env: prod
    owner: user
spec:
  selector:
    app : airflow  # Selecting PODS
  ports:
    - name      : webui # StateStore debug Web UI for administrators to monitor and troubleshoot
      protocol  : TCP
      targetPort: 8080       # Port on POD
      nodePort  : 32088       # Port on Load Balancer
      port      : 3132
  type: NodePort
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-webconf
data:
  webserver_config.py: |-
    # Licensed to the Apache Software Foundation (ASF) under one
    # or more contributor license agreements.  See the NOTICE file
    # distributed with this work for additional information
    # regarding copyright ownership.  The ASF licenses this file
    # to you under the Apache License, Version 2.0 (the
    # "License"); you may not use this file except in compliance
    # with the License.  You may obtain a copy of the License at
    #
    #   http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing,
    # software distributed under the License is distributed on an
    # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
    # KIND, either express or implied.  See the License for the
    # specific language governing permissions and limitations
    # under the License.
    """Default configuration for the Airflow webserver"""
    import os
    from airflow.www.fab_security.manager import AUTH_DB
    # from airflow.www.fab_security.manager import AUTH_LDAP
    # from airflow.www.fab_security.manager import AUTH_OAUTH
    # from airflow.www.fab_security.manager import AUTH_OID
    # from airflow.www.fab_security.manager import AUTH_REMOTE_USER
    basedir = os.path.abspath(os.path.dirname(__file__))

    # Flask-WTF flag for CSRF
    WTF_CSRF_ENABLED = True
    SESSION_COOKIE_NAME = 'session-airflow'
    # ----------------------------------------------------
    # AUTHENTICATION CONFIG
    # ----------------------------------------------------
    # For details on how to set up each of the following authentication, see
    # http://flask-appbuilder.readthedocs.io/en/latest/security.html# authentication-methods
    # for details.

    # The authentication type
    # AUTH_OID : Is for OpenID
    # AUTH_DB : Is for database
    # AUTH_LDAP : Is for LDAP
    # AUTH_REMOTE_USER : Is for using REMOTE_USER from web server
    # AUTH_OAUTH : Is for OAuth
    AUTH_TYPE = AUTH_DB
---
apiVersion: v1
data:
  install-packets.py: |+
    from airflow import DAG
    from datetime import datetime
    import sys
    import os
    sys.path.insert(0,os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))
    from airflow.operators.bash import BashOperator
    from airflow.operators.empty import EmptyOperator
    from airflow.operators.trigger_dagrun import TriggerDagRunOperator
    from airflow.models import DagModel
    import os
    import time
    from airflow import configuration as conf
    from airflow.operators.python import PythonOperator

    #------ параметры ------------
    dagName           = 'install-packets'
    strTags           = "install-packets"
    #-----------------------------

    bash_comm = '''
    cd /tmp \
    && aws s3 cp s3://install/install-packet.tar.gz install-packet.tar.gz --endpoint-url http://minio:9000/ \
    && tar zxf install-packet.tar.gz \
    && /bin/cp -rf ./install-packet/install-packet.py /root/airflow/dags/ \
    && rm -rf /tmp/install-packet \
    && rm -rf /tmp/install-packet.tar.gz \
    && airflow dags reserialize -S /root/airflow/dags/install-packet.py \
    && sleep 20
    '''

    def is_dag_exists(dag_id: str):
        res = False
        while res is False:
            dag = DagModel.get_dagmodel(dag_id)
            if dag is not None:
                res = True
            time.sleep(10)


    CUR_DIR = os.path.abspath(os.path.dirname(__file__))
    with DAG(dag_id=dagName,
            schedule_interval=None,
            is_paused_upon_creation=True,
            start_date=datetime(2023,1,1,0,0),
            catchup=False,
            max_active_runs=1,
            tags=[strTags]) as dag:
        namespace = conf.get('kubernetes', 'NAMESPACE')
        download_task = BashOperator(
            task_id="download_task",
            bash_command=bash_comm
        )
        wait_task = PythonOperator(task_id='wait_task', python_callable=is_dag_exists, op_args=['install-packet-dag'], dag=dag)
        trigger_install = TriggerDagRunOperator(
            task_id="trigger_install_dag",
            trigger_dag_id="install-packet-dag",
            wait_for_completion=True
            )
        complete = EmptyOperator(
            task_id='All_jobs_completed',
            dag=dag)

    download_task >> wait_task >> trigger_install >> complete

kind: ConfigMap
metadata:
  name: install-dags
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  name: spark
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: spark-role
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: spark
  namespace: default
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-run
data:
  run.sh: |-
    #!/bin/bash
    airflow db init
    mkdir -p $AIRFLOW__CORE__DAGS_FOLDER
    cp /opt/install-dags/* $AIRFLOW__CORE__DAGS_FOLDER
    mkdir -p $AIRFLOW__CORE__PLUGINS_FOLDER
    mkdir -p $AIRFLOW__CORE__LOGS_FOLDER
    mkdir -p $AIRFLOW__CORE__PROCESSOR_FOLDER
    mkdir -p $AIRFLOW__CORE__SCHEDULER_FOLDER
    airflow users delete -u admin
    airflow users create --username admin --firstname admin --lastname adminovich --role Admin --email admin@lllal.org --password admin
    airflow standalone
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-init
data:
  init.sh: |-
    #!/bin/bash
    airflow db init
---
apiVersion : apps/v1
kind : Deployment
metadata:
  name: airflow
  labels:
    app : airflow
    project : test
spec:
  selector:
    matchLabels:
      project: test
  template:
    metadata:
      labels:
        app : airflow
        project: test
    spec:
      serviceAccountName: spark
      initContainers:
        - name : init-airflow
          resources: {}
          image : 10.11.4.10:31500/airflow:latest
          imagePullPolicy: IfNotPresent
          command: ["bash", "-c", "/home/airflow/airflow/init.sh"]
          volumeMounts:
            - name: configmap-airflow-init
              mountPath: /home/airflow/airflow/init.sh
              readOnly: true
              subPath: init.sh  
            - name: configmap-airflow-cfg
              mountPath: /home/airflow/airflow/airflow.cfg
              readOnly: false
              subPath: airflow.cfg
      containers:
        - name : airflow
          resources:
            requests:
              memory: "2Gi"
              cpu: 2
            limits:
              memory: "2Gi"
              cpu: 2
          image : 10.11.4.10:31500/airflow:latest
          imagePullPolicy: IfNotPresent
          env:
            - name: GIT_URL
              value: http://gitbucket-int:8080/gitbucket
            - name: PYTHONWARNINGS
              value: ignore::DeprecationWarning,ignore::FutureWarning
            - name: AIRFLOW__CORE__DAGS_FOLDER
              value: /home/airflow/dags
            - name: AIRFLOW__CORE__PLUGINS_FOLDER
              value: /home/airflow/plugins
            - name: AIRFLOW__CORE__LOGS_FOLDER
              value: /home/airflow/logs
            - name: AIRFLOW__CORE__PROCESSOR_FOLDER
              value: /home/airflow/logs/dag_processor_manager/
            - name: AIRFLOW__CORE__SCHEDULER_FOLDER
              value: /home/airflow/logs/scheduler
            - name: IMAGE_NAME_PYSPARK
              value: 10.11.4.10:31500/pyspark:latest
            - name: IMAGE_NAME_KAFKA
              value: 10.11.4.10:31500/kafka_multi:latest
            - name: AWS_ENDPOINT_URL
              value: http://minio:9000
            - name: AWS_ACCESS_KEY_ID
              value: adminminio
            - name: AWS_SECRET_ACCESS_KEY
              value: adminminio
            - name: MLFLOW_URL
              value: http://mlflow:5000/
            - name: MLFLOW_S3_ENDPOINT_URL
              value: http://minio:9000
            - name: GREENPLUM_HOST
              value: greenplum
            - name: GREENPLUM_SCHEMA
              value: ml
            - name: GREENPLUM_PORT
              value: "5432"
            - name: GREENPLUM_DATABASE
              value: sdl
            - name: TARANTOOL_HOST
              value: tarantool
            - name: TARANTOOL_PORT
              value: "3301"
            - name: CLICKHOUSE_HOST
              value: clickhouse
            - name: CLICKHOUSE_PORT
              value: "8123"
            - name: CLICKHOUSE_DATABASE
              value: sdl
          command : ["bash", "-c", "/bin/run.sh"]
          volumeMounts:
            - name: configmap-airflow-run
              mountPath: /bin/run.sh
              readOnly: true
              subPath: run.sh               
            - name: configmap-airflow-cfg
              mountPath: /root/airflow/airflow.cfg
              readOnly: false
              subPath: airflow.cfg
            - name: configmap-airflow-webserver-cfg
              mountPath: /root/airflow/webserver_config.py
              readOnly: false
              subPath: webserver_config.py
            - name: configmap-airflow-install-dags
              mountPath: /opt/install-dags              
      volumes:
       - name: configmap-airflow-install-dags
         configMap:
           defaultMode: 0775
           name: install-dags      
       - name: configmap-airflow-cfg
         configMap:
           defaultMode: 0774
           name: airflowcfg
       - name: configmap-airflow-webserver-cfg
         configMap:
           defaultMode: 0774
           name: airflow-webconf
       - name: configmap-airflow-run
         configMap:
           defaultMode: 0755
           name: airflow-run              
       - name: configmap-airflow-init
         configMap:
           defaultMode: 0755
           name: airflow-init 