{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание файла в формате parquet\n",
    "\n",
    "В этом примере используются библиотеки spark.\n",
    "Библиотеки pandas не возможно использовать на объемах более одного гигабайта данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, LongType, StringType, IntegerType, DateType, TimestampType, FloatType, BooleanType\n",
    "from pyspark.sql.functions import col, cast, date_trunc, sum, dayofweek, hour, dayofmonth, lit, sequence\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# запуск spark\n",
    "spark = SparkSession.builder.appName('parquet').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "schemaData = StructType([ \\\n",
    "    StructField(\"label\",StringType(),True),\n",
    "    StructField(\"feature1\",IntegerType(),True), \n",
    "    StructField(\"feature2\",FloatType(),True), \n",
    "    StructField(\"feature3\",BooleanType(),True), \n",
    "  ])\n",
    "\n",
    "pathParquet = \"/tmp/parquetFiles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# формирование dataframe\n",
    "values= []\n",
    "df = spark.createDataFrame(values,schema=schemaData)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "# запишем пустые данные, необходимо для очистки\n",
    "df.write.mode(\"overwrite\").parquet(pathParquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# убедимся, что папка создана, данных нет\n",
    "dfCount = spark.read.parquet(pathParquet)\n",
    "dfCount.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaIndex = StructType([ \\\n",
    "    StructField(\"indx1\",LongType(),True),\n",
    "    StructField(\"indx2\",LongType(),True),\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем пачку данных\n",
    "# кажется, что гораздо проще создать готовый список и затем обернуть его в spark,\n",
    "# но в отличии от spark простой список не поместится в памяти\n",
    "N = 10\n",
    "indx = list((idx,idx) for idx in range(N))\n",
    "df = spark.createDataFrame(indx, schemaIndex)\n",
    "\n",
    "def extract_features(row):\n",
    " return (\n",
    "    'label'+str(random.randint(1, 10)), #label\n",
    "    random.randint(1, N), #feature1\n",
    "    random.random(), #feature2\n",
    "    bool(random.getrandbits(1)) , #feature3\n",
    " )\n",
    "\n",
    "rdd = df.rdd.map(extract_features)\n",
    "df = rdd.toDF( [\"label\",\"feature1\",\"feature2\", \"feature3\"])\n",
    "df.show()\n",
    "# запись в parquet\n",
    "df.write.mode(\"append\").parquet(pathParquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяем количество\n",
    "dfCount = spark.read.parquet(pathParquet)\n",
    "dfCount.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# увеличиваем набор данных\n",
    "batchSize = 30\n",
    "labelSize = 10\n",
    "def writeParquet():\n",
    "    indx = list((idx,idx) for idx in range(batchSize))\n",
    "    df = spark.createDataFrame(indx, schemaIndex)\n",
    "    def extract_features(row):\n",
    "        return (\n",
    "            'label'+str(random.randint(1, labelSize)), #label\n",
    "            random.randint(1, N), #feature1\n",
    "            random.random(), #feature2\n",
    "            bool(random.getrandbits(1)) , #feature3\n",
    "        )\n",
    "    rdd = df.rdd.map(extract_features)\n",
    "    df = rdd.toDF( [\"label\",\"feature1\",\"feature2\", \"feature3\"])\n",
    "    # запись в parquet\n",
    "    df.write.mode(\"append\").parquet(pathParquet)\n",
    "\n",
    "\n",
    "# делаем некоторое количество пачек\n",
    "for i in range(5):\n",
    "    writeParquet()\n",
    "\n",
    "# проверяем количество\n",
    "dfCount = spark.read.parquet(pathParquet)\n",
    "dfCount.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В папке будет создано множество мелких файлов, количество файлов будет зависеть от числа используемых процессоров, по одному файлу на процессор, плюс несколько итераций. Можно создать один файл. При объединении нужно учесть, один файл может создаваться только одной ниткой процессора, а следовательно время ожидания будет на много выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localPathParquet = \"/tmp/singleparquet\"\n",
    "dfSingle = spark.read.parquet(pathParquet)\n",
    "fileCount = 1 # число файлов\n",
    "dfSingle.coalesce(1).write.mode(\"overwrite\").parquet(localPathParquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# полученный набор данных (dataset) можно закинуть на S3\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "# имя файла в s3\n",
    "filePathParquet = \"/tmp/dataset.parquet\"\n",
    "# имя корзины в s3\n",
    "bucketNameParquet = 'data'\n",
    "\n",
    "s3_target = boto3.resource('s3', \n",
    "    endpoint_url=os.environ[\"AWS_ENDPOINT_URL\"],\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    aws_session_token=None,\n",
    "    config=boto3.session.Config(signature_version='s3v4'),\n",
    "    verify=False\n",
    ")\n",
    "\n",
    "# создание корзины\n",
    "bucket = s3_target.Bucket(bucketNameParquet)\n",
    "if bucket.creation_date:\n",
    "   print(\"The bucket exists\")\n",
    "else:\n",
    "   print(\"The bucket does not exist\")\n",
    "   s3_target.create_bucket(Bucket=bucketNameParquet)\n",
    "\n",
    "# загрузка файла на s3 \n",
    "# в локальной папке находится единственный файл и закидывается на S3\n",
    "print(\"старт записи в s3\")\n",
    "fileNameParquetLocal = [x for x in os.listdir(localPathParquet) if x.endswith(\".parquet\")][0]\n",
    "print('write to s3', 'backet='+bucketNameParquet, 'path='+filePathParquet)\n",
    "s3_target.Bucket(bucketNameParquet).upload_file(localPathParquet+'/'+fileNameParquetLocal, filePathParquet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
