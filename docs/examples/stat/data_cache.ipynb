{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кеширование исходных данных в оперативную память\n",
    "\n",
    "Витрины должны иметь приемлемое время отклика, спарк это медленый инструмент и не подходит для дашбордов. Спарк используется для многопоточной загрузки данных в оперативную память."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формирование исходных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предварительно, необходимо сформировать исходный файл. Пример создания файла ./docs/examples/parquet/create_parquet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, LongType, StringType, IntegerType, DateType, TimestampType, FloatType, BooleanType\n",
    "from pyspark.sql.functions import col, cast, date_trunc, sum, dayofweek, hour, dayofmonth, lit, sequence\n",
    "\n",
    "import random\n",
    "\n",
    "# запуск spark\n",
    "spark = SparkSession.builder.appName('parquet').getOrCreate()\n",
    "\n",
    "\n",
    "schemaData = StructType([ \\\n",
    "    StructField(\"label\",StringType(),True),\n",
    "    StructField(\"feature1\",IntegerType(),True), \n",
    "    StructField(\"feature2\",FloatType(),True), \n",
    "    StructField(\"feature3\",BooleanType(),True), \n",
    "  ])\n",
    "\n",
    "pathParquet = \"/tmp/parquetFiles\"\n",
    "\n",
    "# формирование dataframe\n",
    "values= []\n",
    "df = spark.createDataFrame(values,schema=schemaData)\n",
    "# запишем пустые данные, необходимо для очистки\n",
    "df.write.mode(\"overwrite\").parquet(pathParquet)\n",
    "\n",
    "schemaIndex = StructType([ \\\n",
    "    StructField(\"indx1\",LongType(),True),\n",
    "    StructField(\"indx2\",LongType(),True),\n",
    "  ])\n",
    "\n",
    "# увеличиваем набор данных\n",
    "batchSize = 30\n",
    "labelSize = 10\n",
    "def writeParquet():\n",
    "    indx = list((idx,idx) for idx in range(batchSize))\n",
    "    df = spark.createDataFrame(indx, schemaIndex)\n",
    "    def extract_features(row):\n",
    "        return (\n",
    "            'label'+str(random.randint(1, labelSize)), #label\n",
    "            random.randint(1, N), #feature1\n",
    "            random.random(), #feature2\n",
    "            bool(random.getrandbits(1)) , #feature3\n",
    "        )\n",
    "    rdd = df.rdd.map(extract_features)\n",
    "    df = rdd.toDF( [\"label\",\"feature1\",\"feature2\", \"feature3\"])\n",
    "    # запись в parquet\n",
    "    df.write.mode(\"append\").parquet(pathParquet)\n",
    "\n",
    "\n",
    "# делаем некоторое количество пачек\n",
    "for i in range(5):\n",
    "    writeParquet()\n",
    "\n",
    "localPathParquet = \"/tmp/singleparquet\"\n",
    "dfSingle = spark.read.parquet(pathParquet)\n",
    "fileCount = 1 # число файлов\n",
    "dfSingle.coalesce(1).write.mode(\"overwrite\").parquet(localPathParquet)\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "# имя файла в s3\n",
    "filePathParquet = \"/tmp/dataset.parquet\"\n",
    "# имя корзины в s3\n",
    "bucketNameParquet = 'data'\n",
    "\n",
    "s3_target = boto3.resource('s3', \n",
    "    endpoint_url=os.environ[\"AWS_ENDPOINT_URL\"],\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    aws_session_token=None,\n",
    "    config=boto3.session.Config(signature_version='s3v4'),\n",
    "    verify=False\n",
    ")\n",
    "\n",
    "# создание корзины\n",
    "bucket = s3_target.Bucket(bucketNameParquet)\n",
    "if (not bucket.creation_date):\n",
    "   s3_target.create_bucket(Bucket=bucketNameParquet)\n",
    "\n",
    "# загрузка файла на s3 \n",
    "# в локальной папке находится единственный файл и закидывается на S3\n",
    "fileNameParquetLocal = [x for x in os.listdir(localPathParquet) if x.endswith(\".parquet\")][0]\n",
    "print('write to s3', 'backet='+bucketNameParquet, 'path='+filePathParquet)\n",
    "s3_target.Bucket(bucketNameParquet).upload_file(localPathParquet+'/'+fileNameParquetLocal, filePathParquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кеширование данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pandas\n",
    "\n",
    "bucket_name = 'data'\n",
    "\n",
    "s3_target = boto3.resource('s3', \n",
    "    endpoint_url=os.environ[\"AWS_ENDPOINT_URL\"],\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    aws_session_token=None,\n",
    "    config=boto3.session.Config(signature_version='s3v4'),\n",
    "    verify=False\n",
    ")\n",
    "\n",
    "# чтение файла с s3 файла tmp/data.parquet в локальный файл \n",
    "localPath = '/tmp/data.parquet'\n",
    "remotePath = 'tmp/dataset.parquet'\n",
    "s3_target.Bucket(bucket_name).download_file(remotePath, localPath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, LongType, StringType, IntegerType, DateType, TimestampType, FloatType\n",
    "from pyspark.sql.functions import col, cast, date_trunc, sum, dayofweek, hour, dayofmonth, lit, monotonically_increasing_id\n",
    "\n",
    "# запуск spark\n",
    "spark = SparkSession.builder.appName('fill').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInput = spark.read.parquet(localPath)\n",
    "df = dfInput.select(monotonically_increasing_id().alias('id'.upper()), \n",
    "                    col('label').alias('label'.upper()), \n",
    "                    col('feature1').alias('feature1'.upper()), \n",
    "                    col('feature2').alias('feature2'.upper()), \n",
    "                    col('feature3').alias('feature3'.upper()) ) \n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подключение к БД Tarantool\n",
    "HOST=os.environ['TARANTOOL_HOST']\n",
    "PORT=os.environ['TARANTOOL_PORT']\n",
    "LOGIN=os.environ['TARANTOOL_LOGIN']\n",
    "PASSWORD=os.environ['TARANTOOL_PASSWORD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для tarantol необходимо создавать таблицу\n",
    "tableName = 'dataset'\n",
    "\n",
    "import tarantool\n",
    "print(\"подключение к tarantool\")\n",
    "connection = tarantool.connect(host = HOST, port=PORT, user=LOGIN, password=PASSWORD)\n",
    "table = tableName.upper()\n",
    "# текст запроса на создание таблицы, поля в верхнем регистре\n",
    "sqlCreateTable = f\"\"\"\n",
    "create table {table} (  \n",
    "    ID int primary key, \n",
    "    LABEL string, \n",
    "    FEATURE1 int, \n",
    "    FEATURE2 number, \n",
    "    FEATURE3 boolean \n",
    ")\"\"\"    \n",
    "\n",
    "print(\"проверяется таблица:\" + table)\n",
    "rst = connection.execute(f\"\"\"SELECT EXISTS (select true from \"_space\" where \"name\" = '{table}')\"\"\")\n",
    "tableExist = rst[0][0]\n",
    "print('таблица'+tableName.upper()+' существует '+str(tableExist))\n",
    "if tableExist: \n",
    "    print(\"удаляется таблица:\"+tableName)\n",
    "    connection.execute(f\"drop table {table}\")\n",
    "print(\"запрос создания: \"+sqlCreateTable)\n",
    "connection.execute(sqlCreateTable)\n",
    "print(\"создана таблица: \"+table)\n",
    "#создание дополнительного индекса\n",
    "createIndexSQL = f'''CREATE INDEX {table}_label ON {table} (label)'''\n",
    "print(createIndexSQL)\n",
    "connection.execute(createIndexSQL)\n",
    "#отключение\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполняем таблицу\n",
    "jdbcURLTarantool = f'jdbc:tarantool://{HOST}:{PORT}?user={LOGIN}&password={PASSWORD}'\n",
    "\n",
    "df.write.format(\"jdbc\").mode(\"append\") \\\n",
    "              .option('url', jdbcURLTarantool) \\\n",
    "              .option('driver', 'org.tarantool.jdbc.SQLDriver') \\\n",
    "              .option(\"dbtable\",tableName.upper()).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# спарк завершает работу\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с кешированными данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# открытие соединения\n",
    "connection = tarantool.connect(host = HOST, port=int(PORT), user=LOGIN, password=PASSWORD)\n",
    "\n",
    "sql = f\"\"\"\n",
    "SELECT ID, LABEL, FEATURE1, FEATURE2, FEATURE3 from {tableName.upper()}\n",
    "\"\"\"\n",
    "rst = connection.execute(sql)\n",
    "df = pandas.DataFrame( rst, columns=['ID', 'LABEL', 'FEATURE1', 'FEATURE2', 'FEATURE3'])\n",
    "display(df)\n",
    "# обязательное закрытие соединения\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# открытие соединения\n",
    "connection = tarantool.connect(host = HOST, port=int(PORT), user=LOGIN, password=PASSWORD)\n",
    "\n",
    "sql = f\"\"\"\n",
    "SELECT distinct label \n",
    "from dataset\n",
    "order by label\n",
    "\"\"\"\n",
    "rst = connection.execute(sql)\n",
    "pdf = pandas.DataFrame( rst, columns=['label'])\n",
    "display(pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = pdf['label'][0]\n",
    "print('выбрана запись: '+ label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# все сложности кода предварительного кеширования данных раскрываются на этом примере\n",
    "# для выбора данных используется sql запрос, что упрощает работу с данными\n",
    "# запрос по индексированному полу в памяти будет выполняться на порядки быстрее, пользователь получит почти мгновенный отклик\n",
    "# управление памятью обработки перекладывается на сторону СУБД, тем самым увеличивается предсказуемость выделения ресурсов\n",
    "# СУБД в большинстве случаев для обработки использует многоботочные алгоритмы в отличии от pandas\n",
    "# в данном случае pandas будет обрабатывать ограниченную выборку данных\n",
    "sql = f\"\"\"\n",
    "SELECT  feature1, feature2\n",
    "from dataset\n",
    "where label = '{label}'\n",
    "order by feature3\n",
    "\"\"\"\n",
    "rst = connection.execute(sql)\n",
    "df = pandas.DataFrame( rst, columns=[ 'feature1', 'feature2'])\n",
    "# обязательное закрытие соединения\n",
    "connection.close()\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df.boxplot(meanline=True, showmeans=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
