{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Примеры чтения данных из таблицы greenplum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чтение средствами psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подключение к базе данных\n",
    "HOST=os.environ['GREENPLUM_HOST']\n",
    "PORT=os.environ['GREENPLUM_PORT']\n",
    "DATABASE=os.environ['GREENPLUM_DATABASE']\n",
    "SCHEMA = os.environ['GREENPLUM_SCHEMA']\n",
    "LOGIN=os.environ['GREENPLUM_LOGIN']\n",
    "PASSWORD=os.environ['GREENPLUM_PASSWORD']\n",
    "\n",
    "connection = psycopg2.connect( host=HOST, port=PORT, database=DATABASE, user=LOGIN, password=PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# имя таблицы \n",
    "tableName = 'table1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 2.0, '2'), (3, 3.0, '3'), (6, 6.0, '6'), (4, 4.0, '4'), (1, 1.0, '1'), (5, 5.0, '5')]\n"
     ]
    }
   ],
   "source": [
    "connection = psycopg2.connect( host=HOST, port=PORT, database=DATABASE, user=LOGIN, password=PASSWORD)\n",
    "sql = f\"\"\"\n",
    "     select * FROM {SCHEMA}.{tableName}\n",
    "\"\"\"\n",
    "cur = connection.cursor()\n",
    "cur.execute(sql)\n",
    "data = cur.fetchall()\n",
    "print(data)\n",
    "connection.commit()\n",
    "cur.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   c1   c2 c3\n",
       "0   2  2.0  2\n",
       "1   3  3.0  3\n",
       "2   6  6.0  6\n",
       "3   4  4.0  4\n",
       "4   1  1.0  1\n",
       "5   5  5.0  5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Перводим в pandas\n",
    "pdf = pandas.DataFrame(data)\n",
    "pdf.columns = [desc[0] for desc in cur.description]\n",
    "display(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# не забываем закрывать соединение\n",
    "connection.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чтение средствами spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, LongType, StringType, IntegerType, DateType, TimestampType, FloatType\n",
    "from pyspark.sql.functions import col, cast, date_trunc, sum, dayofweek, hour, dayofmonth, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://packages.confluent.io/maven/ added as a remote repository with the name: repo-1\n",
      "Ivy Default Cache set to: /home/sdl/.ivy2/cache\n",
      "The jars for the packages stored in: /home/sdl/.ivy2/jars\n",
      "org.tarantool#connector added as a dependency\n",
      "org.apache.spark#spark-streaming_2.13 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      "za.co.absa#abris_2.13 added as a dependency\n",
      "com.typesafe#config added as a dependency\n",
      "org.apache.spark#spark-avro_2.13 added as a dependency\n",
      "org.apache.spark#spark-streaming-kafka-0-10_2.13 added as a dependency\n",
      "com.ibm.icu#icu4j added as a dependency\n",
      "org.apache.commons#commons-lang3 added as a dependency\n",
      "com.oracle.database.jdbc#ojdbc10 added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      "com.clickhouse#clickhouse-jdbc added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a4475061-ed0a-47ad-a91d-29d01d5688d7;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.tarantool#connector;1.9.4 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;3.4.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.4.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.3.2 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.9.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound za.co.absa#abris_2.13;6.3.0 in central\n",
      "\tfound io.confluent#kafka-avro-serializer;6.2.1 in repo-1\n",
      "\tfound org.apache.avro#avro;1.10.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.10.5 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.10.5.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.10.5 in central\n",
      "\tfound org.apache.commons#commons-compress;1.21 in central\n",
      "\tfound io.confluent#kafka-schema-serializer;6.2.1 in repo-1\n",
      "\tfound io.confluent#kafka-schema-registry-client;6.2.1 in repo-1\n",
      "\tfound jakarta.ws.rs#jakarta.ws.rs-api;2.1.6 in central\n",
      "\tfound org.glassfish.jersey.core#jersey-common;2.34 in central\n",
      "\tfound jakarta.annotation#jakarta.annotation-api;1.3.5 in central\n",
      "\tfound org.glassfish.hk2.external#jakarta.inject;2.6.1 in central\n",
      "\tfound org.glassfish.hk2#osgi-resource-locator;1.0.3 in central\n",
      "\tfound io.swagger#swagger-annotations;1.6.2 in central\n",
      "\tfound io.swagger#swagger-core;1.6.2 in central\n",
      "\tfound com.fasterxml.jackson.dataformat#jackson-dataformat-yaml;2.10.5 in central\n",
      "\tfound org.yaml#snakeyaml;1.26 in central\n",
      "\tfound io.swagger#swagger-models;1.6.2 in central\n",
      "\tfound com.google.guava#guava;30.1.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;3.8.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.5.1 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound io.confluent#common-utils;6.2.1 in repo-1\n",
      "\tfound za.co.absa.commons#commons_2.13;1.0.0 in central\n",
      "\tfound com.typesafe#config;1.4.0 in central\n",
      "\tfound org.apache.spark#spark-avro_2.13;3.4.0 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-10_2.13;3.4.0 in central\n",
      "\tfound com.ibm.icu#icu4j;70.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.12.0 in central\n",
      "\tfound com.oracle.database.jdbc#ojdbc10;19.18.0.0 in central\n",
      "\tfound org.postgresql#postgresql;42.5.4 in central\n",
      "\tfound com.clickhouse#clickhouse-jdbc;0.4.1 in central\n",
      ":: resolution report :: resolve 1059ms :: artifacts dl 33ms\n",
      "\t:: modules in use:\n",
      "\tcom.clickhouse#clickhouse-jdbc;0.4.1 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.10.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.10.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.10.5.1 from central in [default]\n",
      "\tcom.fasterxml.jackson.dataformat#jackson-dataformat-yaml;2.10.5 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.5.1 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;30.1.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.ibm.icu#icu4j;70.1 from central in [default]\n",
      "\tcom.oracle.database.jdbc#ojdbc10;19.18.0.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.confluent#common-utils;6.2.1 from repo-1 in [default]\n",
      "\tio.confluent#kafka-avro-serializer;6.2.1 from repo-1 in [default]\n",
      "\tio.confluent#kafka-schema-registry-client;6.2.1 from repo-1 in [default]\n",
      "\tio.confluent#kafka-schema-serializer;6.2.1 from repo-1 in [default]\n",
      "\tio.swagger#swagger-annotations;1.6.2 from central in [default]\n",
      "\tio.swagger#swagger-core;1.6.2 from central in [default]\n",
      "\tio.swagger#swagger-models;1.6.2 from central in [default]\n",
      "\tjakarta.annotation#jakarta.annotation-api;1.3.5 from central in [default]\n",
      "\tjakarta.ws.rs#jakarta.ws.rs-api;2.1.6 from central in [default]\n",
      "\torg.apache.avro#avro;1.10.1 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.21 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.12.0 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.13;3.4.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;3.4.0 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-10_2.13;3.4.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;3.4.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.8.0 from central in [default]\n",
      "\torg.glassfish.hk2#osgi-resource-locator;1.0.3 from central in [default]\n",
      "\torg.glassfish.hk2.external#jakarta.inject;2.6.1 from central in [default]\n",
      "\torg.glassfish.jersey.core#jersey-common;2.34 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.5.4 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.tarantool#connector;1.9.4 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.9.1 from central in [default]\n",
      "\torg.yaml#snakeyaml;1.26 from central in [default]\n",
      "\tza.co.absa#abris_2.13;6.3.0 from central in [default]\n",
      "\tza.co.absa.commons#commons_2.13;1.0.0 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]\n",
      "\torg.apache.spark#spark-avro_2.13;3.2.1 by [org.apache.spark#spark-avro_2.13;3.4.0] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 by [org.slf4j#slf4j-api;2.0.6] in [default]\n",
      "\torg.apache.commons#commons-lang3;3.2.1 by [org.apache.commons#commons-lang3;3.12.0] in [default]\n",
      "\torg.checkerframework#checker-qual;3.5.0 by [org.checkerframework#checker-qual;3.8.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   54  |   0   |   0   |   5   ||   49  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a4475061-ed0a-47ad-a91d-29d01d5688d7\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 49 already retrieved (0kB/12ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('fill').getOrCreate()\n",
    "jdbcURLGreenplum = f'jdbc:postgresql://{HOST}:{PORT}/{DATABASE}?user={LOGIN}&password={PASSWORD}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| c1| c2| c3|\n",
      "+---+---+---+\n",
      "|  5|5.0|  5|\n",
      "|  6|6.0|  6|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# способ 1\n",
    "# в этом варианте все данные из таблицы будут скачены в кеш спарка, после применится условие фильтрации\n",
    "df1 = spark.read.format(\"jdbc\") \\\n",
    "              .option('url', jdbcURLGreenplum) \\\n",
    "              .option('driver', 'org.postgresql.Driver') \\\n",
    "              .option('dbtable',SCHEMA+'.'+tableName).load()\n",
    "df1.where('c1>4').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| c1| c2| c3|\n",
      "+---+---+---+\n",
      "|  6|6.0|  6|\n",
      "|  5|5.0|  5|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# способ 2\n",
    "# в этом варианте в спар поступают уже отфильтрованные данные\n",
    "query=f'''\n",
    "SELECT  *\n",
    "FROM {SCHEMA}.{tableName}\n",
    "WHERE c1>4\n",
    "'''\n",
    "df1 = spark.read.format(\"jdbc\") \\\n",
    "              .option('url', jdbcURLGreenplum) \\\n",
    "              .option('driver', 'org.postgresql.Driver') \\\n",
    "              .option('query',query).load()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   c1   c2 c3\n",
       "0   6  6.0  6\n",
       "1   5  5.0  5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# переводим в pandas\n",
    "display(df1.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# желательно остановить спарк\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
